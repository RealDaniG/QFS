import re
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional
REPO_ROOT = Path(__file__).resolve().parents[1]
SRC_DIR = REPO_ROOT / 'src'
TESTS_DIR = REPO_ROOT / 'tests'
DOCS_DIR = REPO_ROOT / 'docs'
EVIDENCE_DIR = REPO_ROOT / 'evidence'
BASELINE_DIR = EVIDENCE_DIR / 'baseline'
DIAGNOSTIC_DIR = EVIDENCE_DIR / 'diagnostic'
REPORT_PATH = DIAGNOSTIC_DIR / 'QFSV13.5_AUTONOMOUS_AUDIT_REPORT.md'

def ensure_dirs():
    DIAGNOSTIC_DIR.mkdir(parents=True, exist_ok=True)

def run_tests() -> Dict:
    """
    Run pytest with deterministic environment and capture output.
    Returns a dict with keys: exit_code, cmd, output_path.
    """
    env = os.environ.copy()
    env.setdefault('PYTHONHASHSEED', '0')
    env.setdefault('TZ', 'UTC')
    output_path = DIAGNOSTIC_DIR / 'pytest_output.txt'
    cmd = ['python', '-m', 'pytest']
    try:
        proc = subprocess.run(cmd, cwd=REPO_ROOT, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=1800)
        output_path.write_text(proc.stdout, encoding='utf-8')
        exit_code = proc.returncode
    except Exception as e:
        exit_code = -1
        output_path.write_text(f'ERROR running pytest: {e}', encoding='utf-8')
    return {'cmd': ' '.join(cmd), 'exit_code': exit_code, 'output_path': str(output_path.relative_to(REPO_ROOT))}

def parse_pytest_summary(output_text: str) -> Dict:
    """
    Very simple parser to extract numbers of tests, failures, errors.
    """
    summary = {'tests': None, 'failed': None, 'errors': None, 'skipped': None, 'xfailed': None, 'xpassed': None}
    lines = output_text.splitlines()
    for line in lines:
        m = re.search('collected\\s+(\\d+)\\s+items', line)
        if m:
            summary['tests'] = int(m.group(1))
    for line in lines[::-1]:
        if 'failed' in line or 'passed' in line or 'error' in line:
            for key, label in [('failed', 'failed'), ('errors', 'errors'), ('skipped', 'skipped'), ('xfailed', 'xfailed'), ('xpassed', 'xpassed')]:
                m = re.search('(\\d+)\\s+' + label, line)
                if m:
                    summary[key] = int(m.group(1))
            break
    return summary

def read_baseline_evidence() -> Dict:
    """
    Load any baseline evidence files present.
    """
    data = {}
    if BASELINE_DIR.is_dir():
        for p in BASELINE_DIR.glob('*.json'):
            try:
                data[p.name] = json.loads(p.read_text(encoding='utf-8'))
            except Exception as e:
                data[p.name] = {'error': f'could not parse JSON: {str(e)}'}
        for p in BASELINE_DIR.glob('*.txt'):
            try:
                data[p.name] = p.read_text(encoding='utf-8', errors='ignore')
            except Exception as e:
                data[p.name] = f'Error reading file: {str(e)}'
    return data

def walk_top_level() -> Dict[str, List[str]]:
    """
    List top-level directories and main files.
    """
    contents = {'dirs': [], 'files': []}
    for entry in REPO_ROOT.iterdir():
        if entry.name.startswith('.'):
            continue
        if entry.is_dir():
            contents['dirs'].append(entry.name)
        else:
            contents['files'].append(entry.name)
    contents['dirs'].sort()
    contents['files'].sort()
    return contents

def scan_src_modules() -> Dict[str, Dict]:
    """
    Build a simple map of src/ modules: purpose (guess), key classes/functions, imports.
    This is heuristic and does not execute code.
    """
    modules: Dict[str, Dict] = {}
    if not SRC_DIR.is_dir():
        return modules
    for pyfile in SRC_DIR.rglob('*.py'):
        rel = pyfile.relative_to(REPO_ROOT)
        text = pyfile.read_text(encoding='utf-8', errors='ignore')
        classes = re.findall('^\\s*class\\s+([A-Za-z0-9_]+)\\s*[\\(:]', text, flags=re.MULTILINE)
        funcs = re.findall('^\\s*def\\s+([A-Za-z0-9_]+)\\s*\\(', text, flags=re.MULTILINE)
        imports = re.findall('^\\s*import\\s+([A-Za-z0-9_\\.]+)', text, flags=re.MULTILINE)
        from_imports = re.findall('^\\s*from\\s+([A-Za-z0-9_\\.]+)\\s+import', text, flags=re.MULTILINE)
        modules[str(rel)] = {'classes': classes, 'functions': funcs, 'imports': sorted(set(imports + from_imports))}
    return modules

def find_non_deterministic_patterns(modules: Dict[str, Dict]) -> Dict[str, List[str]]:
    """
    Best-effort scan for non-deterministic code patterns in src/.
    """
    patterns = {'float': '\\bfloat\\b', 'math': '\\bmath\\.', 'random': '\\brandom\\.', 'time': '\\btime\\.', 'datetime': '\\bdatetime\\.', 'uuid': '\\buuid\\.', 'os_urandom': '\\bos\\.urandom\\b'}
    findings: Dict[str, List[str]] = {}
    for rel in modules:
        path = REPO_ROOT / rel
        text = path.read_text(encoding='utf-8', errors='ignore')
        hits = []
        for label, pat in sorted(patterns.items()):
            if re.search(pat, text):
                hits.append(label)
        if hits:
            findings[rel] = sorted(set(hits))
    return findings

def component_status(modules: Dict[str, Dict], tests_summary: Dict) -> List[Tuple[str, str, str, str]]:
    """
    Very coarse heuristic status table for key components.
    This will not be perfect but will give the report something concrete to show.
    """
    rows: List[Tuple[str, str, str, str]] = []

    def exists(path: str) -> bool:
        return (REPO_ROOT / path).is_file()

    def status_for(path: str) -> str:
        if exists(path):
            return 'Partially implemented'
        return 'Missing'
    key_components = [('BigNum128 core math', 'src/libs/BigNum128.py'), ('CertifiedMath engine', 'src/libs/CertifiedMath.py'), ('DeterministicTime', 'src/libs/DeterministicTime.py'), ('PQC layer', 'src/libs/PQC.py'), ('HSMF framework', 'src/core/HSMF.py'), ('TokenStateBundle', 'src/core/TokenStateBundle.py'), ('DRV_Packet', 'src/core/DRV_Packet.py'), ('CoherenceLedger', 'src/core/CoherenceLedger.py'), ('QFSV13SDK', 'src/sdk/QFSV13SDK.py'), ('AEGIS API', 'src/services/aegis_api.py'), ('CIR302 Handler', 'src/handlers/CIR302_Handler.py')]
    for name, path in key_components:
        st = status_for(path)
        ev = path if exists(path) else 'N/A'
        note = 'Code present, test coverage unknown' if st != 'Missing' else 'No implementation file found'
        rows.append((name, st, ev, note))
    return rows

def generate_report():
    ensure_dirs()
    top_level = walk_top_level()
    modules = scan_src_modules()
    nondet = find_non_deterministic_patterns(modules)
    test_run_info = run_tests()
    pytest_output_text = (REPO_ROOT / test_run_info['output_path']).read_text(encoding='utf-8', errors='ignore')
    pytest_summary = parse_pytest_summary(pytest_output_text)
    baseline = read_baseline_evidence()
    comp_rows = component_status(modules, pytest_summary)
    lines: List[str] = []
    lines.append('# QFS V13.5 Repository Status Report (Autonomous Audit)')
    lines.append('')
    lines.append('**Generated by:** `scripts/run_autonomous_audit.py`  ')
    lines.append(f'**Repository Root:** `{REPO_ROOT}`  ')
    lines.append(f'**Audit Date:** 2025-12-11  ')
    lines.append('')
    lines.append('---')
    lines.append('')
    lines.append('### 1. Architecture Map')
    lines.append('')
    lines.append('**Top-level directories:**')
    lines.append('')
    for d in top_level['dirs']:
        lines.append(f'- `{d}/`')
    lines.append('')
    lines.append('**Top-level files:**')
    lines.append('')
    for f in top_level['files']:
        lines.append(f'- `{f}`')
    lines.append('')
    if modules:
        lines.append('**Key src/ modules (sample):**')
        lines.append('')
        for rel, info in sorted(modules.items()):
            if not (rel.startswith('src\\libs') or rel.startswith('src\\core') or rel.startswith('src\\sdk') or rel.startswith('src\\services') or rel.startswith('src\\handlers')):
                continue
            cls = ', '.join(info['classes'][:5]) or 'None'
            funcs = ', '.join(info['functions'][:5]) or 'None'
            imps = ', '.join(info['imports'][:5]) or 'None'
            lines.append(f'- `{rel}`')
            lines.append(f'  - Classes: {cls}')
            lines.append(f'  - Functions: {funcs}')
            lines.append(f'  - Imports: {imps}')
        lines.append('')
    else:
        lines.append('No `src/` directory found.')
        lines.append('')
    lines.append('### 2. Completeness Assessment')
    lines.append('')
    lines.append('| Component | Status | Evidence (files/tests) | Notes |')
    lines.append('|-----------|--------|------------------------|-------|')
    for name, status, ev, note in comp_rows:
        lines.append(f'| {name} | {status} | `{ev}` | {note} |')
    lines.append('')
    lines.append('_Note: This table is heuristic and should be refined by linking to actual tests and evidence artifacts._')
    lines.append('')
    lines.append('### 3. Determinism & Math Layer Compliance')
    lines.append('')
    if nondet:
        lines.append('The following modules contain potentially non-deterministic patterns (keyword matches):')
        lines.append('')
        for rel, labels in sorted(nondet.items()):
            labels_str = ', '.join(labels)
            lines.append(f'- `{rel}` – patterns: {labels_str}')
        lines.append('')
    else:
        lines.append('No obvious non-deterministic patterns (float/math/random/time/uuid/os.urandom) detected in `src/`.')
        lines.append('')
    lines.append('**Tests summary from this run:**')
    lines.append('')
    lines.append(f"- Command: `{test_run_info['cmd']}`")
    lines.append(f"- Exit code: `{test_run_info['exit_code']}`")
    lines.append(f"- Pytest output: `{test_run_info['output_path']}`")
    lines.append(f'- Parsed summary: {json.dumps(pytest_summary)}')
    lines.append('')
    lines.append('_To fully complete this section, link specific formulas and tests to requirements in `AUDIT-V13.txt`._')
    lines.append('')
    lines.append('### 4. Security & PQC Audit')
    lines.append('')
    pqc_path = REPO_ROOT / 'src' / 'libs' / 'PQC.py'
    if pqc_path.is_file():
        lines.append(f'- PQC implementation found at `src/libs/PQC.py`.')
        lines.append('  - Manual review required to confirm use of real Dilithium-5/Kyber and deterministic serialization.')
    else:
        lines.append('- `src/libs/PQC.py` not found – PQC layer appears missing or located elsewhere.')
    lines.append('')
    lines.append('_A full security audit should scan for eval/exec/unsafe deserialization and verify key management and PQC logging against `AUDIT-V13.txt`._')
    lines.append('')
    lines.append('### 5. Integration Readiness')
    lines.append('')
    sdk_path = REPO_ROOT / 'src' / 'sdk' / 'QFSV13SDK.py'
    api_path = REPO_ROOT / 'src' / 'services' / 'aegis_api.py'
    if sdk_path.is_file():
        lines.append(f'- QFS SDK found at `src/sdk/QFSV13SDK.py` (end-to-end flow entrypoint candidate).')
    if api_path.is_file():
        lines.append(f'- API gateway found at `src/services/aegis_api.py`.')
    lines.append('')
    lines.append('End-to-end integration (DRV_Packet → SDK → HSMF → Treasury/Reward → Ledger) must be verified by dedicated integration tests and deterministic replay evidence.')
    lines.append('')
    lines.append('### 6. Critical Missing Pieces')
    lines.append('')
    lines.append('This section must be refined by comparing code/tests/evidence with `ROADMAP-V13.5-REMEDIATION.md` and `AUDIT-V13.txt`.')
    lines.append('Examples of likely critical items (to be checked):')
    lines.append('')
    lines.append('- HSM/KMS key management layer and tests (`src/security/`, `tests/security/`).')
    lines.append('- SBOM generation and reproducible build scripts (`scripts/generate_sbom.py`, `scripts/build_reproducible.sh`, CI workflows).')
    lines.append('- Oracle attestation framework and quorum rules (`src/oracles/`, oracle tests).')
    lines.append('- Multi-node replication / consensus determinism tests (`src/replication/`, `tests/replication/`).')
    lines.append('- Time regression → CIR-302 tests (`tests/deterministic/test_time_regression_cir302.py`).')
    lines.append('')
    lines.append('_Replace or extend this list after a more detailed comparison with the roadmap and audit guide._')
    lines.append('')
    lines.append('### 7. Final Verdict')
    lines.append('')
    verdict = 'Partially ready'
    lines.append(f'**Verdict:** {verdict}')
    lines.append('')
    lines.append('**Reasoning (based on evidence):**')
    lines.append('')
    lines.append('- Core modules appear present but test results and non-determinism scan indicate further verification is required.')
    lines.append('- Baseline evidence (if present) should be compared with this run to detect regressions.')
    lines.append(f"- Test execution exit code: {test_run_info['exit_code']} (0 = pass, non-zero = issues)")
    lines.append('')
    lines.append('**Suggested next actions (high-level):**')
    lines.append('')
    lines.append('1. Link each requirement in `AUDIT-V13.txt` to specific tests and evidence files, then update this report accordingly.')
    lines.append('2. Implement and run deterministic replay and CIR-302 tests for time regression and economic edge cases.')
    lines.append('3. Build HSM/KMS, SBOM, and reproducible build infrastructure as described in `ROADMAP-V13.5-REMEDIATION.md` and add evidence.')
    lines.append('4. Add integration tests for DRV_Packet → SDK → HSMF → Treasury → Ledger flows and capture deterministic replay artifacts.')
    lines.append('5. Enhance this script to parse more evidence JSONs (e.g., `QFSV13_FULL_COMPLIANCE_AUDIT_REPORT.json`) and include their results directly.')
    lines.append('')
    lines.append('---')
    lines.append('')
    lines.append('### Appendix: Baseline Evidence Snapshot')
    lines.append('')
    if baseline:
        lines.append('Baseline evidence files detected in `evidence/baseline/`:')
        lines.append('')
        for name, content in sorted(baseline.items()):
            if isinstance(content, dict):
                summary_keys = list(content.keys())[:5]
                lines.append(f'- `{name}` – keys: {summary_keys}')
            else:
                lines.append(f'- `{name}` – text length: {len(str(content))}')
        lines.append('')
    else:
        lines.append('No baseline evidence files detected in `evidence/baseline/`.')
        lines.append('')
    REPORT_PATH.write_text('\n'.join(lines), encoding='utf-8')
if __name__ == '__main__':
    generate_report()